<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="description" content="by Sheera Frenkel and Cecilia Kang" />
    <title>An Ugly Truth</title>
    <style>
      html, body {
        margin: 0;
        padding: 3% ;
        font-family: 'Lato';
        font-weight: 500;
        font-size: 12px;
        background: #f2efe9;
        -webkit-print-color-adjust: exact;
        box-sizing: border-box;
      }

      .page {
        position: relative;
        height: 90mm;
        width: 50mm;
        display: block;
        background: #f2efe9;
        page-break-after: always;
        margin: 10px;
        overflow: hidden;
      }

      @media print {
        body {
          background: #f2efe9;
        }

        .page {
          margin: 0;
          height: 100%;
          width: 100%;
        }
      }

      .page.first {
        border-left: 5px solid green;
      }

      .bottom {
        position: absolute;
        left: 5mm;
        right: 5mm;
        bottom: 5mm;
      }

      .group {
        margin-top: 3mm;
        page-break-inside: avoid;
      }

      .line {
        color: #f2efe9;
        position: relative;
      }

      .center {
        text-align: center;
      }

      .logo {
        position: relative;
        width: 80%;
        left: 10%;
        top: 15%;
      }

    </style>
</head>
<body>
    <div class="page">
        <h1 class="center">An Ugly Truth</h1>
    </div>

        <h1 id="whats-in-it-for-me-a-revealing-look-at-a-social-media-empire">What’s in it for me? A revealing look at a social media empire.</h1>
<p>Facebook’s meteoric rise was a wonder to behold. In a mere decade, this dorm-room project expanded from a small campus curiosity to a worldwide social media empire. Yet, in the last few years, the company’s profile has taken a darker turn. The platform is now noted for privacy issues, misinformation, and unsettling political alliances.
These blinks provide an in-depth look into the complex machinations that transformed Facebook into one of the world’s most controversial companies. Based on rigorous reporting, this account delves into how and why the platform became mired in scandal after scandal. Packed with shocking facts and disturbing truths, this timely examination shows that the social network may have been rotten from the start.
In these blinks, you’ll learn</p>
<p>why Zuckerberg ended his first project, FaceMash;
how Facebook disrupted life in Myanmar; and
why the tech company employs a “ratcatcher.”</p>
<h1 id="from-the-very-start-zuckerberg-valued-engagement-over-ethics">From the very start, Zuckerberg valued engagement over ethics.</h1>
<p>December 8, 2015. A new video appears on Facebook. In the short clip, Donald Trump, then just one of many presidential hopefuls, is delivering a fiery speech. He rails against terrorists, against immigrants, and then he calls for a total and complete ban of Muslims entering the United States.
The clip goes viral – within hours it’s been shared 14,000 times and accumulates more than 100,000 likes. For many employees at Facebook, Trump’s anti-Muslim rhetoric is hate speech, a clear violation of the site’s terms and conditions. They want it removed from the site.
Mark Zuckerberg disagrees. After meeting with Joel Kaplan, the VP of public policy, Zuckerberg deems the speech too “newsworthy” to delete. The clip remains up to be shared even more.
The key message here is: From the very start, Zuckerberg valued engagement over ethics.
Even as a Harvard undergrad, Zuckerberg’s approach to social networking caused controversy. In fact, his first platform, FaceMash, was a short-lived blog designed to rank the attractiveness of his female classmates. It was popular, but not with everyone. The site received so much criticism from student groups that Zuckerberg decided to develop a new, less shocking project – Thefacebook.
Launched in 2004, this simple, early iteration of what we now call Facebook only had a few features. It let students set up personal pages, connect with other users, and leave each other messages. Still, it was a massive hit on campuses. By 2005, the platform had more than one million members and most were logging in more than four times a day. The site’s success prompted Zuckerberg to leave Harvard, move to Palo Alto, and run Facebook as a full-time endeavor.
In these early years, Facebook grew in leaps and bounds and the site was feted as Silicon Valley&#39;s next big thing. The hype was so strong that in 2006 Yahoo tried to buy the company for a billion dollars. Zuckerberg turned down the offer. Despite being shy, awkward, and young, he had ambitious plans for the company. Rather than focusing on profits, he aimed for growth. He continually pushed his small number of staff to make the site more engrossing and entertaining.
In September 2006, Facebook launched the News Feed. This new feature gave users a centralized hub displaying activity from all their friends. At first, the Feed was unpopular – users were put off by the information overload and sudden lack of privacy. Yet, Facebook’s metrics told a different story. The Feed made people log on longer and share more – exactly what Zuckerberg wanted.</p>
<h1 id="sandberg-transformed-facebook-into-an-advertising-powerhouse">Sandberg transformed Facebook into an advertising powerhouse.</h1>
<p>Zuckerberg was never a big fan of small talk. Yet, in December 2007, he still worked up the gumption to attend a Christmas party thrown by a colleague from Yahoo. But Zuckerberg didn’t attend for the holiday cheer. No, there was someone there he wanted to meet – a woman named Sheryl Sandberg.
Sandberg already had a sterling reputation as a shrewd businesswoman. Her impressive résumé boasted degrees from Harvard and a stint at the World Bank. At the time, she was working as a VP at Google, perhaps Silicon Valley’s premiere start-up.
At the party, the two talked business for more than an hour. And, over the coming weeks, they met several more times. By March 2008, Facebook proudly named Sandberg its new chief operating officer.
The key message here is: Sandberg transformed Facebook into an advertising powerhouse.
In many ways, Sandberg was exactly what Facebook needed. While Zuckerberg was obsessed with optimizing the site’s technology and user features, he was less concerned with other details, like profits. On the other hand, Sandberg was business-minded. At Google, she’d cultivated the company’s small advertising operation into a multibillion-dollar enterprise. Now, she’d do the same for Facebook.
As Sandberg saw it, Facebook was uniquely suited for the world of online advertising. While Google sold ads based on users’ search terms, Facebook had access to a much wider variety of user data. With this information, the company could serve up targeted ads based on user activity. Not only that, the site&#39;s interactive nature encouraged users to respond directly to companies and share advertisements with friends.
To leverage these advantages, Facebook took steps to efficiently monetize its users&#39; data. In 2009, it introduced the “like” button. This feature let users easily react to anything posted on the site. Facebook then used “likes” to deliver targeted content and, more importantly, collect user preferences to sell to advertisers. The site also adjusted its privacy settings. The new settings were opaque and confusing and tricked users into sharing more information.
Privacy advocates like the Center for Digital Democracy took note of Facebook’s growing exploitation of user data. In December 2009, it filed a complaint with the Federal Trade Commission, or FTC. The filing resulted in Facebook agreeing to regular privacy audits, but, in the coming years, the government did very little to actually monitor or regulate the company’s actions.</p>
<h1 id="facebook-tried-and-failed-to-remain-politically-neutral">Facebook tried and failed to remain politically neutral.</h1>
<p>Officially, Sonya Ahuja was an engineer. But, unofficially, she was “the ratcatcher.” Her role at Facebook was simple – whenever an embarrassing story about the company appeared in the press, she had to identify and fire whoever leaked the info.
For the ratcatcher, 2016 was a busy year. Gizmodo, a popular tech blog, was publishing a string of incriminating articles on Facebook’s internal strife. The reporting alleged that as the American election heated up, users’ News Feeds increasingly featured false stories and incendiary hate speech. According to Gizmodo, Facebook employees wanted to curb this disturbing trend.
Of course, the reporting was all true. In its quest for domination, Facebook had become a major player in political discourse – and the company struggled with its new role.
The key message here is: Facebook tried and failed to remain politically neutral.
By 2016, millions of people in the US and beyond were using Facebook as their primary source of news and information. This was good for the company’s bottom line but came with complications. For one, Facebook’s News Feed algorithm favored posts with high engagement – and, unfortunately, those posts were usually inflammatory and sensationalistic. Users often saw content that confirmed their own biases, no matter how false.
To adjust the tenor of the site, the company introduced “Trending Topics.” This feature let Facebook’s content team partially curate what appeared in users’ feeds. In May 2016, Gizmodo published an article claiming the site used this tool to suppress posts with right-wing viewpoints. Predictably, the conservative media ran wild with the story. Republicans already suspected the site had a liberal bias and this seemed to be proof.
To quell the backlash, Zuckerberg met with major conservative figures like Glenn Beck of Blaze TV and Arthur Brooks of the American Enterprise Institute. In these meetings, the Facebook founder reaffirmed his commitment to free speech and claimed the company was doing all it could to remain politically neutral. The ploy only partially worked – conservatives remained suspicious and liberals resented the appeasement.
Meanwhile, Facebook’s threat intelligence team noticed a troubling new trend. It seemed that Russian hackers were using the site to post misinformation about Democratic candidates. While this wasn’t technically against the site’s rules, some hackers were also distributing emails and other information stolen from the Democratic National Committee. The team shut down some of these rogue accounts, but the damage was done – many of the posts became national news.</p>
<h1 id="facebook-avoided-taking-responsibility-for-widespread-election-meddling">Facebook avoided taking responsibility for widespread election meddling.</h1>
<p>Donald Trump&#39;s unlikely electoral victory took much of the country by surprise – and Facebook was no exception. In the days following the election, Zuckerberg and his team scrambled to come to terms with the new reality.
For one, the company had to grapple with the new, possibly hostile, administration. So, in a bid to secure ties with the new president, the company hired Trump’s former campaign manager, Corey Lewandowski, as a consultant.
For many employees, the thought of working with Trump was a surreal and unappetizing notion. Yet, in the coming months, an even more disturbing idea would emerge. As Facebook investigated its handling of the election, it became more and more clear the company may have helped put him in the White House.
The key message here is: Facebook avoided taking responsibility for widespread election meddling.
In the months following the election, Alex Stamos, Facebook’s head of cybersecurity, launched Project P. This internal investigation attempted to determine if outside actors had used the site as a tool of political propaganda. Stamos’s team poured through thousands of political ads purchased during the election. They looked for any patterns suggesting coordinated political campaigns. Their search led them to the Internet Research Agency, or IRA.
Located in St. Petersburg, the IRA is an organization dedicated to pushing Russian political interests. During the 2016 campaign, it spent more than $100,000 on Facebook ads pushing extreme positions both left and right. The ads were widely liked and shared, eventually reaching 126 million Americans. It’s likely this operation, and others like it, had some sway shaping the election&#39;s news cycle and outcome.
At first, Facebook attempted to downplay these findings – the company was reluctant to get embroiled in a political scandal. Still, in March 2018, a scandal arrived. The New York Times reported that Cambridge Analytica, a UK consulting firm, had used a security hole to surreptitiously acquire data on up to 87 million Facebook users. The firm then sold the data to the Trump campaign to use in designing targeted political advertising.
The revelation reignited concerns about Facebook’s privacy practices. The company’s stock price dropped 10 percent and Zuckerberg was called to testify before Congress. Yet, during the hearing, many congresspeople appeared technologically illiterate and Zuckerberg deftly avoided any incriminating statements. By the end of the day, Facebook’s stock price had recovered and it seemed the company would avoid any real accountability.</p>
<h1 id="facebooks-lax-content-moderation-contributed-to-real-life-violence">Facebook’s lax content moderation contributed to real-life violence.</h1>
<p>It’s August 2017. Sai Sitt Thway Aung, a soldier from the 99th Light Infantry Division of the Burmese military, switches on his phone and logs onto Facebook. Carefully, he types out a new post. It describes his anger at Muslims and his desire to drive them from his country.
Aung isn’t alone, either. All across Myanmar, people are posting and sharing negative messages about the Rohingya, the country’s Muslim minority. Soon, the hate spills over into violence. In the following months, more than 24,000 Rohingya are killed and hundreds of thousands more flee to Bangladesh as refugees.
Later, a United Nations fact-finding team studies the conflict. They deem that Facebook played a “determining role” in escalating the region’s racial tensions into a full-blown genocide.
The key message here is: Facebook’s lax content moderation contributed to real-life violence.
By August 2013, Facebook had amassed more than one billion users, an astounding accomplishment. Still, Zuckerberg had loftier goals. He wanted his platform to host billions more. To reach this figure, the company needed to reach beyond the wealthy, western nations where it was already popular. So, Zuckerberg launched the “Next One Billion,” a project to expand internet access, and Facebook, to developing countries.
While the project succeeded in gaining Facebook users, it also had unintended consequences. The company didn’t anticipate how the platform would be used in new contexts. It also didn’t hire enough staff to moderate the site in the hundreds of new languages it was being used in. So, when radical anti-Muslim Buddhists began spreading anti-Rohingya rhetoric, the company was caught off guard – at least at first.
As early as 2014, human rights activists alerted Facebook about the dangerous hate speech on its platform. Matt Schissler, an activist in Myanmar, even traveled to the company’s headquarters to warn staff about the escalating calls for violence. But his pleas fell on deaf ears. Despite his efforts, Facebook did little to curb the rhetoric and the Rohingya people suffered the consequences.
This avoidable tragedy, along with the Cambridge Analytica scandal and other high-profile missteps, were beginning to erode Facebook’s popularity. Facebook was no longer Silicon Valley’s hottest company, and new talent was seeking work elsewhere. So, in July 2018, Zuckerberg made an announcement. He’d become a “Wartime CEO” and take a bigger role in the company’s day-to-day operations. In the coming year, he’d try to put his company back on track.</p>
<h1 id="facebook-made-many-enemies-with-its-anticompetitive-practices">Facebook made many enemies with its anticompetitive practices.</h1>
<p>By May 2019, Zuckerberg had become accustomed to negative press coverage. But, despite his thick skin, a new op-ed in the New York Times felt like a stab in the back. It was penned by Chris Hughes, one of Facebook’s early cofounders. The title: “It’s Time to Break Up Facebook.”
Hughes had left Facebook a decade earlier and gone on to found the Economic Security Project, a progressive think tank. In his article, he argued that Facebook had grown too big, too fast. He claimed the company crushed competition, played it fast and loose with user data, and in general acted as a dangerous monopoly.
Zuckerberg was right to be disquieted. Hughes was just the latest of many politicians, academics, and consumer advocates calling on the government to dismantle Facebook.
The key message here is: Facebook made many enemies with its anticompetitive practices.
Facebook’s explosive growth was partially fueled by a steadfast policy of buying up smaller competitors. By the time of Hughes’s op-ed, the company had acquired almost 70 other operations. Most of these purchases cost below $100 million, yet there were big mergers, too. In 2012, Facebook bought the photo app Instagram for $1 billion. In 2014, it paid $19 billion to acquire the messenger app WhatsApp.
Such purchases gave Facebook an unprecedented 2.5 billion users worldwide. It also gave the company access to an unfathomable amount of user data. While Zuckerberg originally promised to let each operation keep relative autonomy, he eventually decided to merge the services. While each app would continue to look separate, their back-end infrastructure would actually be deeply interdependent.
For law professors Tim Wu and Scott Hemphill, the merger was merely a strategy to make antitrust action more difficult. By blending each service together, Facebook could claim any government-mandated breakup would be too complicated. Still, the action didn’t change the political landscape. As the 2020 election ramped up, politicians like Elizabeth Warren and Bernie Sanders made regulating Facebook a common talking point.
Meanwhile, Facebook continued to make political missteps. While Zuckerberg and Sandberg bragged about new moderation policies, the site continued to falter. Throughout 2019, controversial deepfake videos circulated on the platform, including an embarrassing video of House Speaker and longtime Silicon Valley ally Nancy Pelosi. When Zuckerberg declined to remove the clip, Pelosi turned against the company. Suddenly, Facebook had very few friends in DC.</p>
<h1 id="facebook-tried-and-failed-to-rebrand-as-a-bastion-of-free-speech">Facebook tried and failed to rebrand as a bastion of free speech.</h1>
<p>Summer 2019 was a busy time for Zuckerberg. At the behest of Joel Kaplan, Nick Clegg, and Facebook&#39;s other public policy experts, the CEO spent months rubbing elbows with political insiders. He met with Republican senator Lindsay Graham, right-wing pundit Tucker Carlson, and finally, President Trump himself.
The meeting with Trump, held in the Oval Office over Diet Cokes, went smoothly. Zuckerberg complimented Trump on his marvelous social media presence. In turn, Trump warmed up to Zuckerberg. Afterward, he even posted a Tweet celebrating the occasion.
While the controversial meeting irked many Facebook employees, Zuckerberg seemed unfazed. After all, if he was going to protect Facebook’s future, he had to have allies in high places.
The key message here is: Facebook faltered to rebrand itself as a bastion of free speech.
By meeting Trump, Zuckerberg wanted to change the conversation about Facebook. He wanted the president, and Republicans in general, to see his business as an asset to America. So, when meeting politicians, he often proclaimed companies like his served as a valuable bulwark against rising competition from Chinese competitors like WeChat and TikTok.
Yet, this wasn’t Zuckerberg’s only strategy. He also wanted to reframe Facebook’s lax moderation as a virtue. In the run-up to the 2020 campaign season, Facebook announced it wouldn’t fact-check or moderate any political advertising whatsoever. Almost immediately, the new policy received backlash from politicians, advocacy groups, and everyday people worried about misinformation marring yet another election.
So, to stave off criticism, Zuckerberg delivered a high-profile speech at Georgetown University’s campus in Washington, DC. In the speech, the CEO touted Facebook’s long-standing commitment to free speech. He falsely claimed the site was founded to cultivate political discussion about the 2003 Iraq War and made bizarre comparisons between posting online and the Civil Rights Movement.
The event was roundly panned by nearly everybody including the Anti-Defamation League and popular congresswoman Alexandria Ocasio-Cortez. In subsequent weeks, Sandberg was forced to defend Zuckerberg’s new position in an interview with journalist Katie Couric. Despite her efforts, she failed to make a better case for Facebook’s new policies. After years of blunders, people were truly tired of the company’s negative effects – and more trials were yet to come.</p>
<h1 id="multiple-crises-push-facebook-to-reevaluate-its-free-speech-absolutism">Multiple crises push Facebook to reevaluate its free speech absolutism.</h1>
<p>President Trump is at it again – he’s standing at the podium blustering his way through another press conference. Only this time, it’s April 2020 and COVID-19 is surging around the world. The president, in a signature digression, suggests ingesting household disinfectants could cure the virus.
Of course, within minutes the clip appears on Trump’s Facebook page. Technically, the post violates the site’s dictum against medical misinformation. Since the start of the pandemic, the company has been more vigilant about suppressing dangerous rumors. Yet, how should it handle this one? Should it delete the President’s post?
Once again, Zuckerberg and his team err on the side of free speech. The post remains up, misinformation and all.
The key message here is: Multiple crises push Facebook to reevaluate its free speech absolutism.
The spring of 2020 marked a sea change for online content moderation. The COVID-19 pandemic and nationwide protest in the wake of George Floyd’s murder kicked off a new wave of heated rhetoric – and platforms were starting to respond. In May, Trump posted a Tweet hinting that protesters should be shot. In an unprecedented response, Twitter labeled the post as dangerous.
Still, Zuckerberg declined to take any action at all. Facebook employees were outraged. Many participated in a digital walkout and 33 founding employees signed an open letter condemning the company. By June, advertisers were also fed up with the negative press – major companies like Verizon, Starbucks, and Ford even staged a one-month boycott of the site.
Trump wasn’t the only issue. Facebook’s new private group feature was wreaking havoc as well. These unmoderated hubs had become cesspools gestating hate speech, conspiracy theories, and right-wing militia organizations. Then, on January 6, a violent mob stormed the Capitol building in an attempt to overturn the recent election. The media quickly noted that many participants spent ample time posting on Facebook and organizing in unmoderated group pages.
Finally, the company changed course. Facebook announced a more stringent policy against dangerous posts and suspended Trump&#39;s account for several weeks. The company even instituted The Facebook Oversight Board, a new, independent panel to rule on content issues. Ostensibly, the board will provide more oversight, though critics allege it’s merely a move to displace responsibility from the company’s executives. For Facebook, the future still remains uncertain.</p>
<h1 id="final-summary">Final summary</h1>
<p>The key message in these blinks is that:
Since its founding, Facebook has constantly courted controversy. Mark Zuckerberg’s emphasis on constant growth, addictive user engagement algorithms, and lax moderation policies transformed the site into an ideal amplifier for extremists views and political discord. While the company has made moves to rein in its worst excesses, much work remains to be done. Whether the social media empire will improve its services or fade away remains an open question.</p>

        <div class="chapter-divider"></div>

</body>
</html>
